version: "3.9"

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: chatbot-backend
    ports:
      - "5000:5000"
    environment:
      - BASE_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - ADAPTER_PATH=/app/tinyllama-qlora-finetuned
    volumes:
      - ./backend/tinyllama-qlora-finetuned:/app/tinyllama-qlora-finetuned:ro
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: chatbot-frontend
    ports:
      - "8080:80"
    depends_on:
      - backend
    # Optional: mount frontend source for live dev (remove in production)
    volumes:
      - ./frontend:/app
